{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Лекция_2_парсинг вакансий HH_ДЗ_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTA8ZfrjbVOQyk65yC2dTh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIexndr/API/blob/main/%D0%9B%D0%B5%D0%BA%D1%86%D0%B8%D1%8F_2_%D0%BF%D0%B0%D1%80%D1%81%D0%B8%D0%BD%D0%B3_%D0%B2%D0%B0%D0%BA%D0%B0%D0%BD%D1%81%D0%B8%D0%B9_HH_%D0%94%D0%97_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCsr_ZYKAshT"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from pprint import pprint\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# функция аргументы - адрес-хедер-парам \n",
        "def get_body(address, headers, params):\n",
        "    page = requests.get(address, headers=headers, params=params)\n",
        "    result = page.text\n",
        "    pprint(result)\n",
        "    link = page.url\n",
        "# на выходе - линк и текст на стр\n",
        "    return link, result\n",
        "\n",
        "#функция вакансий\n",
        "def get_vacs_HH(page):\n",
        "    # делаем пустые списки\n",
        "    list_names = []\n",
        "    list_links = []\n",
        "    list_minzp = []\n",
        "    list_maxzp = []\n",
        "    list_sources = []\n",
        "    source = 'HeadHunter'\n",
        "    soup = bs(page, 'lxml')\n",
        "# библиотекой б-с  ищем через 'div'\n",
        "    block_vacs = soup.find('div', {'class' : 'vacancy-serp'})\n",
        "\n",
        "    for vac in block_vacs:\n",
        "        list_sources.append(source)\n",
        "        head = vac.find('div', { 'class': 'vacancy-serp-item__row vacancy-serp-item__row_header'})\n",
        "        if head is None:\n",
        "            continue\n",
        "        \n",
        "        a = head.find('a')\n",
        "\n",
        "        list_names.append(a.getText())\n",
        "        \n",
        "        list_links.append(a['href'])\n",
        "         \n",
        "        zp = vac.find ('div', { 'class' : 'vacancy-serp-item__compensation'})\n",
        "        if zp is None:\n",
        "            minzp = 'Нет данных'\n",
        "            maxzp = ''\n",
        "            list_minzp.append(minzp)\n",
        "            list_maxzp.append(maxzp)\n",
        "             \n",
        "            continue\n",
        "        zp_full = zp.getText().replace('\\xa0','')\n",
        "         \n",
        "        if zp_full[0:2] == 'от':\n",
        "            minzp = re.findall('\\d*', zp_full)[3]\n",
        "            maxzp = ''\n",
        "            list_minzp.append(minzp)\n",
        "            list_maxzp.append(maxzp)\n",
        "             \n",
        "        elif zp_full[0:2] == 'до':\n",
        "            minzp = ''\n",
        "            maxzp = re.findall('\\d*', zp_full)[3]\n",
        "            list_minzp.append(minzp)\n",
        "            list_maxzp.append(maxzp)\n",
        "             \n",
        "        else:\n",
        "            list_zp = re.split('\\-', zp_full)\n",
        "            minzp = list_zp[0]\n",
        "            maxzp = list_zp[1]\n",
        "            list_minzp.append(minzp)\n",
        "            list_maxzp.append(maxzp)\n",
        "     # на выходе списки названия, линков, зарп мин, макс, источник        \n",
        "    return list_names, list_links, list_minzp, list_maxzp, list_sources\n",
        "\n",
        "#назв вакансии\n",
        "name_vac = 'Data Scientist'\n",
        "# страницы\n",
        "pages= list('1,2,3,4,5')\n",
        "num_pages = [pages[x] for x in range(0, len(pages), 2)]\n",
        "#адрес сайта\n",
        "address = 'https://hh.ru/search/vacancy'\n",
        "list_num=[]\n",
        "#хедер\n",
        "headers={\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36 OPR/63.0.3368.71\"}\n",
        "\n",
        "names = []\n",
        "linki = []\n",
        "mins = []\n",
        "maxs = []\n",
        "istochniki = []\n",
        "\n",
        "for page in num_pages:\n",
        "    params = {\"L_is_autosearch\": \"false\", \"area\": \"1\", \"clusters\": \"true\", \"text\": name_vac, \"page\": page}\n",
        "    url, body = get_body(address, headers, params)\n",
        "    name, vlink, min, max, istochnik = get_vacs_HH(body)\n",
        "    names = names+name\n",
        "    linki = linki+vlink\n",
        "    mins = mins + min\n",
        "    maxs = maxs + max\n",
        "    istochniki = istochniki + istochnik\n",
        "# данные в види дата-сета пандас\n",
        "dataframe_na_export = pd.DataFrame(list(zip(names, linki, mins, maxs, istochniki)), columns=['Имя', 'Ссылка',\n",
        "                                                                                             'Мин. зп','Макс. зп',\n",
        "#запись в таблицу Эксель на диск                                                                                             'Источник'])\n",
        "dataframe_na_export.to_excel('output1.xlsx')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}